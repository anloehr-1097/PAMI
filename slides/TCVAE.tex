\documentclass{beamer}
\usepackage[right]{eurosym}
\usepackage{latexsym}
\usepackage{pgf,pgfarrows,pgfnodes,pgfautomata,pgfheaps}
\usepackage{color}
\usepackage{bbm}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
% \usepackage[vmargin=25mm, top=20mm, bottom=25mm, left=28mm, right=28mm, includehead]{geometry}
% \usepackage{parskip}
\usepackage{csquotes}
\usepackage{german}
\usepackage{ngerman}
\usepackage{microtype}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{extarrows}
\usepackage{amsthm}
\usepackage{bookmark}
\usepackage{mathrsfs}
\usepackage{scrextend}
\usepackage{tikz}
\usepackage{subcaption}
\usepackage{float}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage[singlelinecheck=false,justification=justified]{caption}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{algpseudocode}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{times}
\usepackage[T1]{fontenc}


\graphicspath{{./graphics/}}

\usepackage[
    left = \flqq{},%
    right = \frqq{},%
    leftsub = \flq{},%
    rightsub = \frq{} %
]{dirtytalk}


\bibliographystyle{acm}

\newcommand{\uz}{\wegde}
\newcommand{\oz}{\vee}
\newcommand*\xor{\mathbin{\oplus}}
\everymath{\displaystyle}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\source}[1]{\caption*{Source: {#1}} }
\captionsetup[figure]{font=footnotesize}
\usepackage{commath}
\usepackage{esdiff}
\DeclareMathOperator{\Var}{\mathbf{Var}}
\DeclareMathOperator{\EW}{\mathbf{E}}
\DeclareMathOperator{\WS}{\mathbf{P}}
\DeclareMathOperator{\Cov}{\mathbf{Cov}}
\newcommand{\notimplies}{\;\not\!\!\!\implies}
% Set up

\mode<presentatio>
{
  \usetheme{Warsaw}
  \setbeamercovered{transparent}
}
\setbeamersize{text margin right = 20mm, text margin right= 20mm}
% Oder was auch immer. Zu beachten ist, das Font und Encoding passen
% m�ssen. Falls T1 nicht funktioniert, kann man versuchen, die Zeile
% mit fontenc zu l�schen.

\title[]{TC-VAE: Uncovering Out of distribution generative factors}
\author{Andreas Loehr}
\institute{Goethe Universität Frankfurt a.M.}
\date{
  \vspace{0.2cm}
  Seminar Pattern Analysis and Machine Intelligence
  \vspace{0.4cm}
  \newline 06/22/2023\\
  \vspace{0.3cm} % 0.4cm
  }
\subject{Informatik}

\let\definition\relax
\let\theorem\relax
%\let\footnoterule\relax
% \let\theorem\relax
\theoremstyle{definition}
\newtheorem{definition}[section]{Definition}
\newtheorem{intuition}{Intuition}
% \newtheorem{definition_theorem}[definition]{Definition und Satz}

\newtheorem{cus_theorem}[section]{Satz}
%\newtheorem{def_theorem}[section]{Definition und Satz}
% \newtheorem{lemma}[definition]{Lemma}
% \newtheorem{remark}[definition]{Bemerkung}
% \newtheorem{remark_ex}[definition]{Beispiel}
% \newtheorem{notation}[definition]{Notation}
\newtheorem{remark}{Bemerkung}
\begin{document}
  \AtBeginSection[]
  {
      \begin{frame}
          \tableofcontents[currentsection]
      \end{frame}
    }
    % title page
  \begin{frame}
    \begin{titlepage}
    \end{titlepage}
  \end{frame}
  \begin{frame}
    \frametitle{Structure of the presentation (info sheet)}
    \begin{itemize}
      \item background/ preliminary work
      \item approach
      \item results
      \item discussion and conclusion
    \end{itemize}
  \end{frame}
  %intro to generative modelling. The problem statement
  \section{Representation Learning and Generative Modeling}
    \begin{frame}
      \frametitle{The Problem statement}
      \begin{itemize}
        \setlength{\itemindent}{-2em}
        \item Problem: Given data $X \in \mathbb{R}^{d}$ find a \textit{good} latent representation $Z \in \mathbb{R}^{m}$, $d, m \in \mathbb{N}$
        \item Example: $X$ images of colored 3D objects, $Z$ latent representation representing shape, texture, color etc.
      \end{itemize}
     \end{frame}

     \begin{frame}
      \frametitle{Example: 3D shapes dataset}
      \begin{figure}
        \centering
        \includegraphics[scale=0.15]{3d_shapes.png}
        \captionsetup{justification=centering}
        \caption*{\tiny{Source: https://github.com/deepmind/3d-shapes/tree/master}}
      \end{figure}
      %\vspace{-0.8 cm}
      \begin{itemize}
        \setlength{\itemindent}{-2em}
        \item Generative Factors $=\{[floor, wall, object] color, scale, shape, orientation\}$
      \end{itemize}
     \end{frame}

      \begin{frame}
        \frametitle{Definitions and Notation}
        \begin{itemize}
          \item \textbf{Data Generative Factors:} True underlying factors / attributes of the data or generation process
          \item \textbf{OOD Data Generative Factors:} Generative Factors without variability in dataset
          % explain: no variation of this factor in entire dataset, e.g. all objects are located in center of image for OOD factor camera perspective
          \item \textbf{Latent Representation:} The vector of latent variables
        \end{itemize}
      \end{frame}

    \begin{frame}
      \frametitle{What is a good learned latent representation?}
      \begin{itemize}
        \item captures the true generative factors of the dataset
        \item interpretable
              $\rightarrow$ Optimally, single component of latent representation represents single generative factor
        \item sufficiently informative (captures majority of data generative factors)
        \item balanced
        \item Regularity
        \item TODO: completeness definition
        %, i.e. generative factors underrepresented in training data still have correspoding latent factor factors underrepresented in training data still have correspoding latent factor factors underrepresented in training data still have correspoding latent factor
        % in the sense that even generative factors which are underrepresented in the dataset do have a correspoding latent factor
      \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Why bother to learn good representations?}
        \begin{itemize}
          \item Facilitate downstream tasks like classification
          \item Generative modeling: control generative factors explictly by tweaking the corresponding latent variables
          % TODO: add more reasons and illustrate
        \end{itemize}
      \end{frame}

    \section{(Variational) Autoencoders}
    \begin{frame}
      \frametitle{Means to learn latent representations}
      \begin{itemize}
        \item Principal component analysis (PCA)? $\rightarrow$ Lack of interpretability
        \item (Regular) Autoencoders? $\rightarrow$ Lack of regularity
        % explain regularity - refer to Medium article/ paper
        % https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73
        \item Variational Autoencoders (VAE)?
      \end{itemize}
    \end{frame}

    \begin{frame}
      \frametitle{Autoencoders}
      % source autoencoder illustration
      %https://miro.medium.com/v2/resize:fit:4266/1*QEmCZtruuWwtEOUzew2D4A.png
      %\begin{center}
      \begin{figure}
        \includegraphics[scale= 0.09]{Autoencoder_illustration.png}
        \caption*{Source: https://medium.com/autoencoder-for-anomaly-detection/autoencoder-for-anomaly-detection-db6178ad07b2}
        % explain graphics quickly
      \end{figure}
      % \end{center}

    \end{frame}
    \begin{frame}
      \frametitle{Variational Autoencoders}
      \begin{figure}
        \includegraphics[scale=.125]{vae-diagram.jpg}
        \caption{Source: TODO}
      \end{figure}
      \vspace{-5mm}
      \begin{itemize}
        \item Generative model consisting of probabilisitic encoder and decoder
        \item Encoder \& decoder parametrize probability distributions over latent space and data respectively
      \end{itemize}
      % explain the graphics

    \end{frame}
    \begin{frame}
      \frametitle{Training Variational Autoencoders}
      Inspecting Encoder and Decoder
      The variational lower bound (ELBO) as on objective and lower bound to the likelihood of data
    \end{frame}

    \begin{frame}
      \frametitle{Shortcomings}
    \end{frame}

    \begin{frame}
      \frametitle{Trying to Fix VAEs}
      $\beta$-VAEs and alternatives
    \end{frame}


    \section{TC-VAEs}
    \begin{frame}
      \frametitle{Prerequisites}
      \begin{itemize}
        \item $X = (X_{1}, \dots, X_{d})$ random variable (rv) w/ density $p(x)$ and distribution $P^{X}$
        \item $Z = (Z_{1}, \dots, Z_{m})$ rv w/ density $p(z)$ and distribution $P^{Z}$
      \end{itemize}
    \end{frame}

    \begin{frame}
      \frametitle{Shannon Entropy}
      \begin{definition}[Shannon Entropy]
        $H(X) \coloneqq -\mathbb{E}_{P^{X}}[\log p(x)] = -\int_{\R^{d}}p(x) \log p(x) dx$
      \end{definition}
      % Note: def for rv continous, in discrete case, replace Rd by N and dx by counting measure
      % Note: def is equivalent for conditional distributions
      \begin{intuition}
       Measures the randomness of a random variable.
     \end{intuition}
     \begin{example}
       \begin{itemize}
         \item Dirac Measure in a point $\rightarrow H(X) = 0$
               \item Coin Flip $\rightarrow H(X) = 1$
       \end{itemize}
     \end{example}
    \end{frame}


    \begin{frame}
      \frametitle{Mutual Information}
      \begin{definition}[Mutual Information]
        $I(X, Z) \coloneqq H(X) + H(Z) - H(X, Z) = H(X) - H(X \mid Z)$
      \end{definition}
      % Note: def for rv continous, in discrete case, replace Rd by N and dx by counting measure
      \begin{intuition}
        Reduction in uncertainty of one rv given another rv.
        % Or what is left actually. Understand this
      \end{intuition}
    \end{frame}

    \begin{frame}
      \frametitle{Kullback-Leibler Divergence}
      \begin{definition}[Kullback-Leibler Divergence ($D_{KL}$)]
        $I(X, Z) \coloneqq H(X) + H(Z) - H(X, Z) = H(X) - H(X \mid Z)$
      \end{definition}
      % Note: def for rv continous, in discrete case, replace Rd by N and dx by counting measure
      \begin{intuition}
        Reduction in uncertainty of one rv given another rv.
        % Or what is left actually. Understand this
      \end{intuition}
    \end{frame}

    \begin{frame}
      \frametitle{Total Correlation}
      \begin{definition}[Total Correlation (TC)]
        $TC(X) \coloneqq definition here$
      \end{definition}
      \begin{intuition}
      \end{intuition}
      TODO: Total correlation def and intuition
    \end{frame}

    \begin{frame}
      \frametitle{Total Correlation as an objective}
      TODO: Maximizing $TC(Z) - TC(Z \mid X)$. equivalence to minimizing the second term. Equivalence to $Z$ being independent conditioned on $X$.
    \end{frame}


    \begin{frame}
      \frametitle{Information Bottleneck and Deep Variational IB}
      TODO: Definition and Intuition
    \end{frame}


    \begin{frame}
      \frametitle{A new objective}
      \begin{itemize}
        \item Using TC as an objective
        \item Combination of different terms promoting different behavior (VIB, CEB, reconstruction error)
        \item Explanation of the single terms, given an intuition
      \end{itemize}
    \end{frame}

    \begin{frame}
      \frametitle{A convex lower bound}
      Presenting the TC convex lower bound actually used in optimization
    \end{frame}


  \section{Experiments and Results}
    \begin{frame}
      \frametitle{Experiment Design}
      \begin{itemize}
        \item datasets + image of data to illustrate
        \item baseline models
        \item evaluation methods (i.e. downstream scores, qualitative (visual))
      \end{itemize}
    \end{frame}

    \begin{frame}
      \frametitle{Results - Qualitative}
      \begin{itemize}
        \item Traversals through latent space show discovery of 2 OOD generative factors in 3D objects dataset
      \end{itemize}
      \begin{figure}
        \centering
        \includegraphics[scale=0.2]{latent_traversals_TC.png}
        \captionsetup{justification=centering}
        \caption*{\tiny{Source: original paper}}
        % understand how this image is generated, how do traversals work?

      \end{figure}
    \end{frame}

    \begin{frame}
      \frametitle{Results - Quantitative}
      \begin{itemize}
        \item On datasets using abovementionEd metrics.
        \item Stress the performance on unbalanced datasets
      \end{itemize}
    \end{frame}

  \section{Discussion and Conclusion}
    \begin{frame}
      \frametitle{Discussion and Conclusion}
      \begin{itemize}
        \item Inspiration from Multiview representation learning (OOD generative factors correspond to missing views)
        \item Uncovering OOD generative factors $\equiv$ inferring missing views
        \item Experimental validation that TC-VAE is capable of uncovering OOD generative factors
        \item missing view also problematic for downstream tasks. underrepresented views may be seen as outliers
        \item main shortcoming: Terms appearing as part of the bound do promote conflicting behavior during minization
        \item unbalanced generative factors in dataset $\implies$ disentanglement deteriorates.
      \end{itemize}
    \end{frame}


    \begin{frame}
      \frametitle{Criticism}
      \begin{enumerate}
              \item The concept of disentanglement is not clearly defined. Sometimes its equivalent to independence, sometimes not.
      \end{enumerate}
    \end{frame}
    \begin{frame}
      \frametitle{Literature}
      Book Elements of information theory
    \end{frame}
   \end{document}
