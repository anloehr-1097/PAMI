\documentclass[twoside,11pt]{article}

\usepackage{blindtext}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

% Available options for package jmlr2e are:
%
%   - abbrvbib : use abbrvnat for the bibliography style
%   - nohyperref : do not load the hyperref package
%   - preprint : remove JMLR specific information from the template,
%         useful for example for posting to preprint servers.
%
% Example of using the package with custom options:
%
% \usepackage[abbrvbib, preprint]{jmlr2e}

\usepackage{jmlr2e}
% Definitions of handy macros can go here

\usepackage[T1]{fontenc}
\usepackage[right]{eurosym}
\usepackage{latexsym}
\usepackage{pgf,pgfarrows,pgfnodes,pgfautomata,pgfheaps}
\usepackage{color}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[vmargin=25mm, top=20mm, bottom=25mm, left=28mm, right=28mm, includehead]{geometry}
\usepackage{parskip}
\usepackage{csquotes}
\usepackage{german}
\usepackage{ngerman}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{extarrows}
\usepackage{bookmark}
\usepackage{mathrsfs}
\usepackage{scrextend}
\usepackage{tikz}
\usepackage{subcaption}
\usepackage{float}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage[singlelinecheck=false,justification=justified]{caption}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{algpseudocode}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{graphicx}
% \usepackage{times}
\usepackage{mathptmx}
\usepackage{calrsfs}

\DeclareMathAlphabet{\pazocal}{OMS}{zplm}{m}{n}
\newcommand{\La}{\mathcal{L}}
\newcommand{\Lb}{\pazocal{L}}
\graphicspath{{./graphics/}}

\usepackage[
    left = \flqq{},%
    right = \frqq{},%
    leftsub = \flq{},%
    rightsub = \frq{} %
]{dirtytalk}


\newcommand{\uz}{\wegde}
\newcommand{\oz}{\vee}
\newcommand*\xor{\mathbin{\oplus}}
\everymath{\displaystyle}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\source}[1]{\caption*{Source: {#1}} }
\captionsetup[figure]{font=footnotesize}
\usepackage{commath}
\usepackage{esdiff}
\DeclareMathOperator{\Var}{\mathbf{Var}}
\DeclareMathOperator{\EW}{\mathbf{E}}
\DeclareMathOperator{\WS}{\mathbf{P}}
\DeclareMathOperator{\Cov}{\mathbf{Cov}}
\newcommand{\notimplies}{\;\not\!\!\!\implies}
\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\usepackage{lastpage}
\jmlrheading{23}{2022}{1-\pageref{LastPage}}{1/21; Revised 5/22}{9/22}{21-0000}{Author One and Author Two}

% Short headings should be running head and authors last names

\ShortHeadings{TC-Vae: Uncovering Out-of-Distribution Data Generative Factors} {A brief summary}
\firstpageno{1}

\begin{document}

\title{TC-VAE: Uncovering Out-of-Distribution Data Generative Factors - A summary}

\author{Authors of the original paper: Christian Meo, Anirudh Goyal, Justin Dauwels}
\editor{Andreas Loehr}
\maketitle


\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
  Learning disentangled representations of a given data distribution represents an integral part of unsupervised learning.
  One goal is to uncover the true generative factors of the data distribution.
  To this day, noone was able to uncover data generative factors with little to no variablity (OOD generative factors) in the dataset presented.
  Moreover, the evaluation of the models learning such representations has been carried out on datasets with perfectly balanced generative factors.
  The paper's contribution to the current body of research is threefold. The authors respond to the abovementioned shortcomings by introducing a new generative model named TC-VAE which is optimized on a convex lower bound of the total correlation between the learned representation and the data and by comparing the performance of this model with common baseline models on standard datasets used for the task, as well as a custom designed dataset with unbalanced data generative factors. They illustrate their proposed model's capability to uncover OOD generative factors. To improve the qualitative evaulation of the perfomance, they propose a new method used to visualize latent space traversals.
\end{abstract}

\section{Introduction}
\subsection{Motivation and background}
A \textit{good} latent representation of a given data distribution oftentimes yields a solid foundation for generative tasks as well as downstream tasks in supervised learning. It is widely accepted that learning such representations in an unsupervised fashion benefits a variety of subfields of machine learning [TODO BETTER INTRO HERE].
The goodness of a representation is characterized in terms of its degree of disentanglement. A representation is said to be disentangled if each component in the learned latent vector corresponds to a single generative factor of the data. Even though a unified formal / mathematical definition of disentanglement is nowhere to be found, different definitions of disentaglment of various authors do converge and seem to capture the same concept.

Various attempts have been made to generate disentagled representations. The model architectures predominantly being inspired by that of a Variational Auto Encoder (VAE), the idea in most approaches is to modify the optimization objective such that the distribution of the latent vector conditioned on the data factorizes.
Quantification of the learned latent representation (distribution) is usually done on toy datasets with a small amount of known generative factors. The evaluation is done qualitatively inspecting the latent space traversals obtained from manipulating single latent variables of a representation and quantitatively leveraging a set of metrics which are designed to capture the degree of disentangled of a representation.

\subsection{Contributions}
The contributions of the authors of the paper are threefold. On the one hand, they propose a new objective function to minimize during the training of a VAE. Their derived objective is a convex combination of 2 existing objectives which do have their roots in information theory and in theory promote learning disentagled representations.
They demonstrate that, using their new objective in VAE training, their model is capable of uncovering generative factors which are not captured in the dataset - so called \enquote{Out of Distribution} (OOD) generative factors.
On the other hand, they design a new dataset used for the evaluation of models learning data representations. Their dataset is only a slight modification of an existing dataset. More explicitly, they create an unbalanced version of the dataset, one generative factor being not uniformly distributed over the dataset as is common in the existing datasets.
Moreover, they evaluate existing models on their newly designed dataset, examining the effects of the disbalance on the representations learned.

\section{Background and Preliminary Work}
For the remainder of the paper we use the following notation.
\begin{itemize}
  \item Let $X \in \R^{d}, Z \in \R^{m}, \hspace{0.1 cm} d,m \in \N$. Let $P^{X}, P^{Z}$ or $p(x), p(z)$ the distributions of $X$ respectively $Z$. As long as it is clear for the context, we might use $p(z)$ for the distribution as well as the density.
  \item We call $X$ the data and $Z$ the latent representation. Usually, $m << d$. Latent representations are of much smaller dimension than the data.
  \item Conditional distributions are denoted as $P^{Z \mid X}$ or $p(z \mid x)$.
\end{itemize}
A major difficulty is to find a precise definition of disentanglement. Due to the lack of one such definition, a commonly used one is as follows.

\begin{definition}[Disentanglement \cite{Bengio_2013}]
  A latent representation is called \textit{disentagled} if there is a one-to-one correspondence between latent variables and generative factors, i.e. a change in 1 latent variable correponds to a change in 1 latent variable and vice versa.
\end{definition}
We may break down the concept of disentaglement in terms of
\begin{itemize}
  \item \textit{completeness}: A low average number of latent variables is required to captue a single generative factor.
  \item \textit{informativeness}: Each latent variable does capture an entire generative factor.
\end{itemize}

One key concept used throughout the paper to push models towards learning disentagled representations is that of \textit{Total Correlation} of a random vector.

\begin{definition}[Mutual Information]
  TODO
\end{definition}

\begin{definition}[Total Correlation]
  For $Z \in \R^{m}, m \in \N$ we call
  \begin{align*}
    TC(z) \coloneqq D_{KL}(p(z) \mid \prod_{i=1}^{m}p(z_{i}))
  \end{align*}
  the \textit{Total Correlation} of $Z$. Moreover, we define
  \begin{align*}
    TC(z \mid x) \coloneqq D_{KL}(p(z \mid x) \mid \prod_{i=1}^{m}p(z_{i} \mid x))
  \end{align*}
  as the \textit{Conditional Total Correlation} of $Z$ given $X$.
\end{definition}
Intuitively, total correlation of a random vector captures the amount of information shared among the single components of the vector. Some authors use the concept of total correlation to define disentanglement. Others make clear, that just relying on this concept for the definition of disentanglement does not suffice. (TODO: add sources or explain)

$TC$ is however used as part of the objective function (\cite{FactorVAE}) or as in the paper at hand, as a foundation to derive a more suitable lower bound to optimize in order to learn disentagled representations.





\section{Approach}
The starting point for the derivation of the objective is the joint correlation of the data and the learned representation and a convex lower bound thereof.
\begin{definition}[Joint Total Correlation]
  We define the joint total correlation as
  \begin{align*}
    TC(z, x) \coloneqq TC(Z) - TC(z \mid x)
  \end{align*}
\end{definition}
By breaking down $TC(z, x)$ using concepts from information theory, we may gain introspection into the usefulness of this as an objective to learn disentangled representations.


\begin{proposition}[TC in terms of Mutual Information]
  Here.
\end{proposition}


\begin{proposition}[TC in terms of Conditional Mutual Information]
  Here.
\end{proposition}





\section{Results}

\section{Discussion and Conclusion}



% Acknowledgements and Disclosure of Funding should go at the end, before appendices and references

\acks{All acknowledgements go at the end of the paper before appendices and references.
Moreover, you are required to declare funding (financial activities supporting the
submitted work) and competing interests (related financial activities outside the submitted work).
More information about this disclosure can be found on the JMLR website.}

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

\appendix
\section{}
\label{app:theorem}

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:

In this appendix we prove the following theorem from
Section~6.2:

\noindent
{\bf Theorem} {\it Let $u,v,w$ be discrete variables such that $v, w$ do
not co-occur with $u$ (i.e., $u\neq0\;\Rightarrow \;v=w=0$ in a given
dataset $\dataset$). Let $N_{v0},N_{w0}$ be the number of data points for
which $v=0, w=0$ respectively, and let $I_{uv},I_{uw}$ be the
respective empirical mutual information values based on the sample
$\dataset$. Then
\[
	N_{v0} \;>\; N_{w0}\;\;\Rightarrow\;\;I_{uv} \;\leq\;I_{uw}
\]
with equality only if $u$ is identically 0.} \hfill\BlackBox

\section{}

\noindent
{\bf Proof}. We use the notation:
\[
P_v(i) \;=\;\frac{N_v^i}{N},\;\;\;i \neq 0;\;\;\;
P_{v0}\;\equiv\;P_v(0)\; = \;1 - \sum_{i\neq 0}P_v(i).
\]
These values represent the (empirical) probabilities of $v$
taking value $i\neq 0$ and 0 respectively.  Entropies will be denoted
by $H$. We aim to show that $\fracpartial{I_{uv}}{P_{v0}} < 0$....\\

{\noindent \em Remainder omitted in this sample. See http://www.jmlr.org/papers/ for full paper.}


\vskip 0.2in
\bibliography{sample}

\end{document}
