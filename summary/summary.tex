\documentclass[twoside,11pt]{article}

\usepackage{blindtext}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

% Available options for package jmlr2e are:
%
%   - abbrvbib : use abbrvnat for the bibliography style
%   - nohyperref : do not load the hyperref package
%   - preprint : remove JMLR specific information from the template,
%         useful for example for posting to preprint servers.
%
% Example of using the package with custom options:
%
% \usepackage[abbrvbib, preprint]{jmlr2e}

\usepackage{jmlr2e}
% Definitions of handy macros can go here

\usepackage[T1]{fontenc}
\usepackage[right]{eurosym}
\usepackage{latexsym}
\usepackage{pgf,pgfarrows,pgfnodes,pgfautomata,pgfheaps}
\usepackage{color}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[vmargin=25mm, top=20mm, bottom=25mm, left=28mm, right=28mm, includehead]{geometry}
\usepackage{parskip}
\usepackage{csquotes}
\usepackage{german}
\usepackage{ngerman}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{extarrows}
\usepackage{bookmark}
\usepackage{mathrsfs}
\usepackage{scrextend}
\usepackage{tikz}
\usepackage{subcaption}
\usepackage{float}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage[singlelinecheck=false,justification=justified]{caption}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{algpseudocode}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{graphicx}
% \usepackage{times}
\usepackage{mathptmx}
\usepackage{calrsfs}

\DeclareMathAlphabet{\pazocal}{OMS}{zplm}{m}{n}
\newcommand{\La}{\mathcal{L}}
\newcommand{\Lb}{\pazocal{L}}
\graphicspath{{./graphics/}}

\usepackage[
    left = \flqq{},%
    right = \frqq{},%
    leftsub = \flq{},%
    rightsub = \frq{} %
]{dirtytalk}


\newcommand{\uz}{\wegde}
\newcommand{\oz}{\vee}
\newcommand*\xor{\mathbin{\oplus}}
\everymath{\displaystyle}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\source}[1]{\caption*{Source: {#1}} }
\captionsetup[figure]{font=footnotesize}
\usepackage{commath}
\usepackage{esdiff}
\DeclareMathOperator{\Var}{\mathbf{Var}}
\DeclareMathOperator{\EW}{\mathbf{E}}
\DeclareMathOperator{\WS}{\mathbf{P}}
\DeclareMathOperator{\Cov}{\mathbf{Cov}}
\newcommand{\notimplies}{\;\not\!\!\!\implies}
\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\usepackage{lastpage}
\jmlrheading{23}{2022}{1-\pageref{LastPage}}{1/21; Revised 5/22}{9/22}{21-0000}{Author One and Author Two}

% Short headings should be running head and authors last names

\ShortHeadings{TC-Vae: Uncovering Out-of-Distribution Data Generative Factors} {A brief summary}
\firstpageno{1}

\begin{document}

\title{TC-VAE: Uncovering Out-of-Distribution Data Generative Factors - A summary}

\author{Authors of the original paper: Christian Meo, Anirudh Goyal, Justin Dauwels}
\editor{Andreas Loehr}
\maketitle


\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
  Learning disentangled representations of a given data distribution represents an integral part of unsupervised learning.
  One goal is to uncover the true generative factors of the data distribution.
  To this day, noone was able to uncover data generative factors with little to no variablity (OOD generative factors) in the dataset presented.
  Moreover, the evaluation of the models learning such representations has been carried out on datasets with perfectly balanced generative factors.
  The authors present a new model called the TC-VAE able to learn uncover such OOD generative factors and compare its performance to baseline models on unbalanced datasets.



  % $The paper's contribution to the current body of research is threefold. The authors respond to the abovementioned shortcomings by introducing a new generative model named TC-VAE which is optimized on a convex lower bound of the total correlation between the learned representation and the data and by comparing the performance of this model with common baseline models on standard datasets used for the task, as well as a custom designed dataset with unbalanced data generative factors. They illustrate their proposed model's capability to uncover OOD generative factors. To improve the qualitative evaulation of the perfomance, they propose a new method used to visualize latent space traversals.
\end{abstract}

\section{Introduction}
\subsection{Motivation and background}
A \textit{good} latent representation of a given data distribution oftentimes yields a solid foundation for generative tasks as well as downstream tasks in supervised learning. It is widely accepted that learning such representations in an unsupervised fashion benefits a variety of subfields of machine learning [TODO BETTER INTRO HERE].
The goodness of a representation is characterized in terms of its degree of disentanglement. A representation is said to be disentangled if each component in the learned latent vector corresponds to a single generative factor of the data. Even though a unified formal / mathematical definition of disentanglement is nowhere to be found, different definitions of disentaglment of various authors do converge and seem to capture the same concept.

Various attempts have been made to generate disentagled representations. The model architectures predominantly being inspired by that of a Variational Auto Encoder (VAE), the idea in most approaches is to modify the optimization objective such that the distribution of the latent vector conditioned on the data factorizes.
Quantification of the learned latent representation (distribution) is usually done on toy datasets with a small amount of known generative factors. The evaluation is done qualitatively inspecting the latent space traversals obtained from manipulating single latent variables of a representation and quantitatively leveraging a set of metrics which are designed to capture the degree of disentangled of a representation.

\subsection{Contributions}
The contributions of the authors of the paper are threefold. On the one hand, they propose a new objective function to minimize during the training of a VAE. Their derived objective is a convex combination of 2 existing objectives which do have their roots in information theory and in theory promote learning disentagled representations.
They demonstrate that, using their new objective in VAE training, their model is capable of uncovering generative factors which are not captured in the dataset - so called \enquote{Out of Distribution} (OOD) generative factors.
On the other hand, they design a new dataset used for the evaluation of models learning data representations. Their dataset is only a slight modification of an existing dataset. More explicitly, they create an unbalanced version of the dataset, one generative factor being not uniformly distributed over the dataset as is common in the existing datasets.
Moreover, they evaluate existing models on their newly designed dataset, examining the effects of the disbalance on the representations learned.

\section{Background and Preliminary Work}
For the remainder of the paper we use the following notation.
\begin{itemize}
  \item Let $X \in \R^{d}, Z \in \R^{m}, \hspace{0.1 cm} d,m \in \N$. Let $P^{X}, P^{Z}$ or $p(x), p(z)$ the distributions of $X$ respectively $Z$. As long as it is clear for the context, we might use $p(z)$ for the distribution as well as the density.
  \item We call $X$ the data and $Z$ the latent representation. Usually, $m << d$. Latent representations are of much smaller dimension than the data.
  \item Conditional distributions are denoted as $P^{Z \mid X}$ or $p(z \mid x)$.
\end{itemize}
A major difficulty is to find a precise definition of disentanglement. Due to the lack of one such definition, a commonly used one is as follows.

\begin{definition}[Disentanglement \cite{Bengio_2013}]
  A latent representation is called \textit{disentagled} if there is a one-to-one correspondence between latent variables and generative factors, i.e. a change in 1 latent variable correponds to a change in 1 latent variable and vice versa.
\end{definition}
We may break down the concept of disentaglement in terms of
\begin{itemize}
  \item \textit{completeness}: A low average number of latent variables is required to captue a single generative factor.
  \item \textit{informativeness}: Each latent variable does capture an entire generative factor.
\end{itemize}
The authors take an information-theoretic approach to the derivation of their lower bound of the likelihood of the data. We thus introduce a range of concepts essential to the understanding of the results of the paper. Note that for all of the following definitions, an analogous definition can be provided for the \enquote{conditional} case by replacing the distributions by their conditional counterparts.

\begin{definition}[Shannon Entropy]
  The entropy of a random variable is defined as
  \begin{align*}
    H(X) \coloneqq - \mathbb{E}_{P^{X}}[\log(p(X))] = - \int_{\R^{d}}\log p(x)p(x) dx,
  \end{align*}
  where $p(x)$ is the density of $X$.
\end{definition}

\begin{definition}[Mutual Information (\cite{Gao_2019})]
  Mutual information of two random variables is defined as
  \begin{align*}
    I(Z, X) \coloneqq H(X) + H(Z) - H(X, Z) = H(Z) - H(Z \mid X).
  \end{align*}
\end{definition}

One key concept used throughout the paper to push models towards learning disentagled representations is that of \textit{Total Correlation} of a random vector.

\begin{definition}[Total Correlation]
  For $Z \in \R^{m}, m \in \N$ we call
  \begin{align*}
    TC(Z) \coloneqq D_{KL}(p(z) \mid \prod_{i=1}^{m}p(z_{i})).
  \end{align*}
  the \textit{Total Correlation} of $Z$. Moreover, we define
  \begin{align*}
    TC(Z \mid X) \coloneqq D_{KL}(p(z \mid x) \mid \prod_{i=1}^{m}p(z_{i} \mid x)).
  \end{align*}
  as the \textit{Conditional Total Correlation} of $Z$ given $X$.
\end{definition}

\begin{remark}[Interpretation]
  The concepts can be interpreted as follows:
  \begin{itemize}
    \item The entropy of a random variable quantifies the randomness/ uncertainty in its distribution.
    \item Mutual information of 2 rvs yields the reduction of uncertainty in one random variable given the other random variable.
    \item TC measures the amount of information shared among the rvs. It is a generalization of MI to more than 2 rvs.
  \end{itemize}
\end{remark}

 Some authors use TC to define disentanglement. Others make clear, that just relying on this concept for the definition of disentanglement does not suffice. (TODO: add sources or explain)

$TC$ is however used as part of the objective function (\cite{FactorVAE}) or as in the paper at hand, as a foundation to derive a more suitable lower bound to optimize in order to learn disentagled representations.

\section{Approach}
The starting point for the derivation of the objective is the joint correlation of the data and the learned representation and a convex lower bound thereof.
\begin{definition}[Joint Total Correlation]
  We define the joint total correlation as
  \begin{align*}
    TC(Z, X) \coloneqq TC(Z) - TC(Z \mid X)
  \end{align*}
\end{definition}
By breaking down $TC(z, x)$ using concepts from information theory, we may gain introspection into the usefulness of this as an objective to learn disentangled representations.


\begin{proposition}[TC in terms of Mutual Information]
  \begin{align*}
    TC(Z, X) = (\sum_{i=1}^{m}I(Z_{k}, X)) - I(Z,X).
  \end{align*}
\end{proposition}
Thus maximizing $TC(Z,X)$ is equivalent to maximizing the information shared between single components $Z_{i}$ of $Z$.
Moreover, by writing $I(Z,X) = I(Z) - I(Z \mid X)$ we see that at the same time, independence of $Z$ given $X$ is promoted.
In the literature $I(Z,X)$ is called the \enquote{Variation Information Bottleneck}(VIB).

\begin{proposition}[TC in terms of Conditional Mutual Information \cite{Gao_2019}]
  \begin{align*}
    TC(z, x) = \frac{1}{m}\sum_{i=1}^{m}(K-1)I(Z_{i}, X) - I(Z_{\neq i}, X \mid Z_{i}).
  \end{align*}
\end{proposition}
From this representation, we see that maximizing joint total correlation is equivalent to maximizing the terms of the form $I(Z_{\neq i}, X \mid Z_{i})$. Intuitively this promotes balance of information related to each single variable. This term is called the \enquote{Conditional Information Bottleneck}(CEB).

\begin{remark}[How maximizing TC yields disentangled representations]
  TODO
\end{remark}

\begin{definition}[Convex combination lower bound]
  TODO derivation of final bound used in optimization
\end{definition}


\section{Experiments \& Results}
\begin{itemize}
  \item baselines, datasets, evaluation metrics
\end{itemize}

\section{Discussion and Conclusion}
The authors demonstrated TC-VAE's capability of detecting OOD generative factors. They raised awareness of the need to further investigate the effects of unbalanced datasets on models' capabilities to learn disentagled representations.
A first step in this direction was taken in the evaluation of baseline models as well as the new model on the custom-designed unbalanced 3D-shapes dataset.



% Acknowledgements and Disclosure of Funding should go at the end, before appendices and references

\acks{All acknowledgements go at the end of the paper before appendices and references.
Moreover, you are required to declare funding (financial activities supporting the
submitted work) and competing interests (related financial activities outside the submitted work).
More information about this disclosure can be found on the JMLR website.}

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

\appendix
\section{}
\label{app:theorem}

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:

In this appendix we prove the following theorem from
Section~6.2:

\bibliography{sample}

\end{document}
